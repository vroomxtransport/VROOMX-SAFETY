---
phase: 06-dataq-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/models/Company.js
  - backend/services/dataQAnalysisService.js
autonomous: true

must_haves:
  truths:
    - "Company syncStatus schema can store dataQAnalysisLastRun timestamp"
    - "dataQAnalysisService has a bulk analysis method for processing multiple violations"
  artifacts:
    - path: "backend/models/Company.js"
      provides: "dataQAnalysisLastRun and dataQAnalysisCount in syncStatus"
      contains: "dataQAnalysisLastRun"
    - path: "backend/services/dataQAnalysisService.js"
      provides: "runBulkAnalysis method for post-sync analysis"
      exports: ["runBulkAnalysis"]
  key_links:
    - from: "backend/services/dataQAnalysisService.js"
      to: "Violation model"
      via: "query for recently synced violations"
      pattern: "syncMetadata.importedAt"
---

<objective>
Add DataQ analysis infrastructure for post-sync processing

Purpose: Enable automated DataQ opportunity scoring after each FMCSA sync, so newly imported violations immediately appear with challenge potential scores in the DataQ opportunities list.

Output: Company schema extended with DataQ tracking field, and dataQAnalysisService with bulk analysis capability.
</objective>

<execution_context>
@/Users/reepsy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/reepsy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-dataq-integration/06-RESEARCH.md

# Key source files
@backend/models/Company.js
@backend/services/dataQAnalysisService.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend Company syncStatus schema with DataQ tracking fields</name>
  <files>backend/models/Company.js</files>
  <action>
Add two new fields to `fmcsaData.syncStatus` in the Company schema:

1. `dataQAnalysisLastRun`: Date - when DataQ analysis last ran for this company
2. `dataQAnalysisCount`: Number - how many violations were analyzed in last run

Also add 'dataq_analysis' to the `source` enum in the errors array.

Location: Inside the existing `syncStatus` object (around line 134-147), add after `linkingLastRun`.

Pattern to follow from existing code:
```javascript
linkingLastRun: { type: Date },
// ADD THESE:
dataQAnalysisLastRun: { type: Date },
dataQAnalysisCount: { type: Number }
```

And update the source enum from:
`enum: ['csa_scores', 'violations', 'inspections', 'entity_linking']`
to:
`enum: ['csa_scores', 'violations', 'inspections', 'entity_linking', 'dataq_analysis']`
  </action>
  <verify>
Run: `cd "/Users/reepsy/Documents/TRUCKING COMPLIANCE HUB1/backend" && node -e "const Company = require('./models/Company'); const schema = Company.schema.obj; console.log('syncStatus fields:', Object.keys(schema.fmcsaData.syncStatus)); console.log('errors source enum:', schema.fmcsaData.syncStatus.errors[0].source.enum)"`

Expected: Should show dataQAnalysisLastRun and dataQAnalysisCount in syncStatus fields, and dataq_analysis in source enum.
  </verify>
  <done>Company schema has dataQAnalysisLastRun and dataQAnalysisCount fields in syncStatus, and dataq_analysis in errors source enum</done>
</task>

<task type="auto">
  <name>Task 2: Add runBulkAnalysis method to dataQAnalysisService</name>
  <files>backend/services/dataQAnalysisService.js</files>
  <action>
Add a new async function `runBulkAnalysis(companyId, options)` that:

1. Queries violations that:
   - Belong to companyId
   - Have `syncMetadata.importedAt` in the last 24 hours (recently synced)
   - Do NOT already have `dataQChallenge.aiAnalysis` (avoid re-analyzing)
   - Have status 'open' or 'upheld'

2. For each violation (up to options.maxViolations, default 50):
   - Run `calculateChallengeScore(violation)` to get local scoring
   - Store the score in `dataQChallenge.aiAnalysis` via `saveAnalysisToViolation()`

3. Return results object: `{ analyzed: number, skipped: number, errors: array }`

Key implementation details:
- Use existing `calculateChallengeScore()` (already in this file)
- Use existing `saveAnalysisToViolation()` (already in this file)
- Do NOT call AI service (aiService.analyzeDataQChallenge) - local scoring only for bulk
- Sort by severityWeight descending to prioritize high-impact violations
- Sequential processing (no parallel) to avoid overwhelming the database

Add to exports: `runBulkAnalysis`

Example implementation pattern (from RESEARCH.md):
```javascript
async function runBulkAnalysis(companyId, options = {}) {
  const { maxViolations = 50 } = options;

  const recentCutoff = new Date(Date.now() - 24 * 60 * 60 * 1000);

  const violations = await Violation.find({
    companyId,
    'syncMetadata.importedAt': { $gte: recentCutoff },
    'dataQChallenge.aiAnalysis': { $exists: false },
    status: { $in: ['open', 'upheld'] }
  })
  .sort({ severityWeight: -1 })
  .limit(maxViolations);

  const results = { analyzed: 0, skipped: 0, errors: [] };

  for (const violation of violations) {
    try {
      const scoreDetails = calculateChallengeScore(violation);

      await saveAnalysisToViolation(violation._id, {
        score: scoreDetails.score,
        factors: scoreDetails.factors,
        deductions: scoreDetails.deductions,
        category: scoreDetails.category,
        confidence: scoreDetails.score >= 75 ? 'high' : scoreDetails.score >= 50 ? 'medium' : 'low'
      });

      results.analyzed++;
    } catch (err) {
      results.errors.push({ violationId: violation._id.toString(), error: err.message });
    }
  }

  return results;
}
```
  </action>
  <verify>
Run: `cd "/Users/reepsy/Documents/TRUCKING COMPLIANCE HUB1/backend" && node -e "const service = require('./services/dataQAnalysisService'); console.log('runBulkAnalysis exported:', typeof service.runBulkAnalysis === 'function')"`

Expected: `runBulkAnalysis exported: true`
  </verify>
  <done>dataQAnalysisService exports runBulkAnalysis function that scores recently-synced violations</done>
</task>

</tasks>

<verification>
1. Company schema has new syncStatus fields:
   - dataQAnalysisLastRun (Date)
   - dataQAnalysisCount (Number)
   - dataq_analysis in errors source enum

2. dataQAnalysisService.runBulkAnalysis(companyId) exists and:
   - Queries violations with recent syncMetadata.importedAt
   - Excludes already-analyzed violations
   - Uses local scoring (not AI)
   - Returns { analyzed, skipped, errors }
</verification>

<success_criteria>
- Schema extension is non-breaking (new optional fields only)
- runBulkAnalysis can be called with a companyId and returns result object
- No AI calls made during bulk analysis (cost control)
- Backend server starts without errors: `cd backend && npm start` (then Ctrl+C)
</success_criteria>

<output>
After completion, create `.planning/phases/06-dataq-integration/06-01-SUMMARY.md`
</output>
